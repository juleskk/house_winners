{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check for necessary \n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import (StandardScaler, MinMaxScaler)\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_curve, auc, log_loss, classification_report)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from operator import itemgetter\n",
    "import pickle\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.plotly as py\n",
    "from plotly.graph_objs import *\n",
    "from scipy import stats\n",
    "from scipy.stats import (median_test, mannwhitneyu)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import (cross_val_score, cross_val_predict)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.externals.six import StringIO\n",
    "import pydot\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import datasets, svm\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import cross_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../datafiles/data.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-133-42675404d103>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-133-42675404d103>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df['leadership'].apply(lambda x: x =1 if x !=0)\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df['leadership', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "money = ['itemproxy','unitemproxy','ieproxy','totalind','instate',\n",
    "         'itemind', 'unitemagainst', 'itemagainst' ,'unitemfor',\n",
    "         'unitemind', 'totalpacs','itempacs','unitempacs',\n",
    "         'ieagainst','itemfor', 'iefor' , 'unitemtotal',  \n",
    "         'itemtotal' ,'total', 'unitemie',  'itemie','percunitem',\n",
    "         'percpacs', 'ie', 'percie','percind','percstate']\n",
    "\n",
    "coms =['ecn' ,'tax',  'bus', 'rul', 'sci', 'tra', 'nat' ,'jud',\n",
    "       'adm' ,'gov' ,'for' ,'ene' ,'edu', 'bud', 'ban' ,'arm',\n",
    "       'app', 'pri', 'egw', 'lib', 'hsc', 'int', 'way', 'vet', \n",
    "       'sta', 'agr']\n",
    "\n",
    "bio = [ 'congyear' , 'feccandid', 'party', 'candname' , 'winner', 'cpo', 'office', 'state', 'district']\n",
    "status = ['leader', 'chairman' ,'power', 'leadershipcom', 'leadership']\n",
    "election = ['close2' , 'close5', 'close10', 'vap',  'generalturnout', \n",
    "            'primaryturnout' ,  'primary' , 'primaryperc' ,  'general','generalperc' ]\n",
    "misc = ['majorityprev', ]\n",
    "\n",
    "drop = ['seat' , 'acceptpacs', 'cid',  'incumbent', 'open' , 'challenger']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################################\n",
    "# Functions\n",
    "###################################\n",
    "\n",
    "# Find optimal number of neighbors for KNN\n",
    "def max_k(X, y, n=20):\n",
    "   \n",
    "    X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "    k_list = []\n",
    "    temp = {}\n",
    "    for i in range(1, n+1):\n",
    "        model = KNeighborsClassifier(n_neighbors=i).fit(X_train, y_train)\n",
    "        temp[i] = accuracy_score(y_test, model.predict(X_test))\n",
    "    n = int(max(temp.iteritems(), key=itemgetter(1))[0]) \n",
    "    kvalue = float(max(temp.iteritems(), key=itemgetter(1))[1])                 \n",
    "    \n",
    "    return n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocessing \n",
    "money_df = df[money] \n",
    "min_max_scaler = MinMaxScaler()\n",
    "scaled = min_max_scaler.fit_transform(money_df)\n",
    "scaled_df = pd.DataFrame(scaled, columns = money_df.columns)\n",
    "\n",
    "          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = scaled_df.join(df[['ecn' ,'tax',  'bus', 'rul', 'sci', 'tra', 'nat' ,'jud',\n",
    "#            'adm' ,'gov' ,'for' ,'ene' ,'edu', 'bud', 'ban' ,'arm',\n",
    "#            'app', 'pri', 'egw', 'lib', 'hsc', 'int', 'way', 'vet',\n",
    "#            'sta', 'agr', 'congyear', 'party', 'winner', 'c', 'i', 'o',\n",
    "#            'leader', 'chairman' ,'power', 'leadershipcom', 'leadership',\n",
    "#            'primaryperc', 'majorityprev']])\n",
    "\n",
    "y = df['winner']\n",
    "X = df[['c', 'i', 'o', 'party']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.85      0.85      0.85       313\n",
      "          1       0.87      0.87      0.87       364\n",
      "\n",
      "avg / total       0.86      0.86      0.86       677\n",
      "\n",
      "Accuracy Score: 0.8580\n",
      "Precision: 0.8680\n",
      "Recall: 0.8680\n",
      "F1: 0.8680\n"
     ]
    }
   ],
   "source": [
    "# Juat train/test split\n",
    "\n",
    "log = LogisticRegression()\n",
    "X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "model = log.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print \"classification report\"\n",
    "print classification_report(y_test, y_pred)\n",
    "print \"Accuracy Score: %.4f\" % round(accuracy_score(y_test, y_pred), 3)\n",
    "print \"Precision: %.4f\" % round(precision_score(y_test, y_pred),3)\n",
    "print \"Recall: %.4f\" % round(recall_score(y_test, y_pred),3)\n",
    "print \"F1: %.4f\" % round(f1_score(y_test, y_pred),3)\n",
    "                           \n",
    "# # models = [LogisticRegression, RandomForestClassifier, KNeighborsClassifier,\n",
    "# #           SVC, DecisionTreeClassifier, GaussianNB, GradientBoostingClassifier]\n",
    "\n",
    "# if algo == KNeighborsClassifier:\n",
    "#     accs = []\n",
    "#     for n in range(1,50):\n",
    "#     model = KNeighborsClassifier(n_neighbors=n).fit(X_train,Y_train)\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     accuracy = accuracy_score(y_test, Y_pred)\n",
    "#     accs.append({n:accuracy})\n",
    "\n",
    "# Predicting 1-0: Logistic, Naive Bayes, SVM, decision trees\n",
    "# Predicting proba: Log, SVM(prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data type not understood",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-7712b587aad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#         for coef in coefs:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#             print '%.05f \\t%s' % (coef)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Julia/anaconda/lib/python2.7/site-packages/numpy/matrixlib/defmatrix.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, data, dtype, copy)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# now convert data to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: data type not understood"
     ]
    }
   ],
   "source": [
    "zip( X_train.columns, [round(x, 3) for x in model.coef_[0]])\n",
    "#  coefs = sorted(zip(m.coef_[0], X_tr.columns))\n",
    "#         for coef in coefs:\n",
    "#             print '%.05f \\t%s' % (coef)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>p</th>\n",
       "      <th>variable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>694.8329</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>566.4009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.9843</td>\n",
       "      <td>0.3211</td>\n",
       "      <td>o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2332</td>\n",
       "      <td>0.6292</td>\n",
       "      <td>party</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       coef       p variable\n",
       "0  694.8329  0.0000        c\n",
       "1  566.4009  0.0000        i\n",
       "2    0.9843  0.3211        o\n",
       "3    0.2332  0.6292    party"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_coefs(X, y):\n",
    "    chi = chi2(X, y)\n",
    "    d = {'variable': list(X.columns.values),\n",
    "    'coef': [round(x,4) for x in chi[0]],\n",
    "    'p': [round(x,4) for x in chi[1]]}\n",
    "pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "0.0001\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.7070 \n",
      "   precision score            0.6654 \n",
      "   recall score               0.9730 \n",
      "   f1 score                   0.7878 \n",
      "\n",
      "\n",
      "0.001\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8555 \n",
      "   precision score            0.8923 \n",
      "   recall score               0.9230 \n",
      "   f1 score                   0.8699 \n",
      "\n",
      "\n",
      "0.01\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8555 \n",
      "   precision score            0.8923 \n",
      "   recall score               0.9230 \n",
      "   f1 score                   0.8699 \n",
      "\n",
      "\n",
      "0.1\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8444 \n",
      "   precision score            0.8367 \n",
      "   recall score               0.8230 \n",
      "   f1 score                   0.7985 \n",
      "\n",
      "\n",
      "1\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8484 \n",
      "   precision score            0.8395 \n",
      "   recall score               0.8182 \n",
      "   f1 score                   0.7994 \n",
      "\n",
      "\n",
      "10\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8484 \n",
      "   precision score            0.8395 \n",
      "   recall score               0.8182 \n",
      "   f1 score                   0.7994 \n",
      "\n",
      "\n",
      "100\n",
      "--------------------\n",
      "\n",
      "   accuracy score             0.8484 \n",
      "   precision score            0.8395 \n",
      "   recall score               0.8182 \n",
      "   f1 score                   0.7994 \n"
     ]
    }
   ],
   "source": [
    "# Choosing penalty for logistic regression\n",
    "c = [.0001, .001, .01, .1, 1, 10, 100]\n",
    "for penalty in c:\n",
    "    log = LogisticRegression(C = penalty)\n",
    "    print '\\n'\n",
    "    print penalty\n",
    "    print '-'*20\n",
    "    print\n",
    "    print '   accuracy score             %.4f ' % np.mean(cross_val_score(log, X,y, cv=10, scoring='accuracy'))\n",
    "    print '   precision score            %.4f ' % np.mean(cross_val_score(log, X,y, cv=10, scoring='precision'))\n",
    "    print '   recall score               %.4f ' % np.mean(cross_val_score(log, X,y, cv=10, scoring='recall'))\n",
    "    print '   f1 score                   %.4f ' % np.mean(cross_val_score(log, X,y, cv=10, scoring='f1'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoresTrain = []\n",
    "scoresTest=[]\n",
    "for i in range(1, 21):\n",
    "    model = DecisionTreeClassifier(max_depth = i)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    scoresTrain.append(accuracy_score(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    scoresTest.append(accuracy_score(y_test, y_test_pred))\n",
    "plotdata = pd.DataFrame(data={'Train': scoresTrain, 'Test': scoresTest})\n",
    "plotdata.index=range(1, 21)\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('ggplot')\n",
    "plotdata.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Build a forest and compute the feature importances\n",
    "forest = ExtraTreesClassifier(n_estimators=250,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X_train, y_train.ravel())\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "#Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(len(features)):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]])), features[float(indices[f])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x119015cd0>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAECCAYAAADq7fyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nhBhiJNiFEGKIkWAXQogh5v8HlLGTmPdUkJIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118fe1b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scoresTrain = []\n",
    "scoresTest=[]\n",
    "for i in range(1, 40):\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    scoresTrain.append(accuracy_score(y_train, y_train_pred))\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    scoresTest.append(accuracy_score(y_test, y_test_pred))\n",
    "    \n",
    "plotdata = pd.DataFrame(data={'Train': scoresTrain, 'Test': scoresTest})\n",
    "plotdata.index=range(1, plotdata.shape[0] + 1)\n",
    "plotdata\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plotdata.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.588</td>\n",
       "      <td>0.735</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.922</td>\n",
       "      <td>0.933</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.535</td>\n",
       "      <td>0.591</td>\n",
       "      <td>KNeighborsClassifier</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.563</td>\n",
       "      <td>0.719</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.910</td>\n",
       "      <td>0.923</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.753</td>\n",
       "      <td>0.749</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.923</td>\n",
       "      <td>0.935</td>\n",
       "      <td>GradientBoostingClassifier</td>\n",
       "      <td>0.926</td>\n",
       "      <td>0.945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Accuracy     F1                       Model  Precision  Recall\n",
       "0     0.588  0.735          LogisticRegression      0.591   0.972\n",
       "1     0.922  0.933      RandomForestClassifier      0.937   0.930\n",
       "2     0.535  0.591        KNeighborsClassifier      0.611   0.573\n",
       "3     0.563  0.719                         SVC      0.578   0.950\n",
       "4     0.910  0.923      DecisionTreeClassifier      0.922   0.925\n",
       "5     0.753  0.749                  GaussianNB      0.933   0.626\n",
       "6     0.923  0.935  GradientBoostingClassifier      0.926   0.945"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "\n",
    "# models = [LogisticRegression, RandomForestClassifier, KNeighborsClassifier,\n",
    "#           SVC, DecisionTreeClassifier, GaussianNB, GradientBoostingClassifier]\n",
    "\n",
    "# results = []\n",
    "# for model in models:\n",
    "#     fit = model().fit(X_train, y_train)\n",
    "#     y_pred = fit.predict(X_test)\n",
    "#     precision = round(precision_score(y_test, y_pred),3)\n",
    "#     recall = round(recall_score(y_test, y_pred), 3)\n",
    "#     f1 = round(f1_score(y_test, y_pred),3)\n",
    "# #     chi = chi2(X, y)\n",
    "# #     coef1 = round(float(chi[0][0]),3)\n",
    "# #     coef2= round(float(chi[0][1]),1)\n",
    "# #     p1 = round(float(chi[1][0]),3)\n",
    "# #     p2 = round(float(chi[1][1]),3)\n",
    "#     accuracy = round(accuracy_score(y_test, y_pred), 3)\n",
    "    \n",
    "#     result = {\n",
    "#         'Model': model.__name__,\n",
    "#         'Precision': precision,\n",
    "#         'Recall': recall,\n",
    "#         'F1': f1,\n",
    "# #         'Coef': coef1,\n",
    "# #         'Coef2': coef2,\n",
    "# #         'P-value': p1,\n",
    "# #         'P-value2': p2, \n",
    "#         'Accuracy': accuracy\n",
    "#     }\n",
    "#     results.append(result)\n",
    "   \n",
    "    \n",
    "# cpo_tot = pd.DataFrame(results)\n",
    "# cpo_tot\n",
    "\n",
    "# ###\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "# classifier = GradientBoostingClassifier()\n",
    "# target_names = ['Winner', 'Loser']\n",
    "# classifier.fit(X_train, y_train)\n",
    "# print model.__name__\n",
    "# print classification_report(y_test, fit.predict(X_test), target_names=target_names)\n",
    "\n",
    "###\n",
    "models = {'log': LogisticRegression(),\n",
    "          'rf': RandomForestClassifier(),\n",
    "          'knn': KNeighborsClassifier(n_neighbors=kbest),\n",
    "          'linsvc': LinearSVC(),\n",
    "          'tree': DecisionTreeClassifier(),\n",
    "          'gnb': GaussianNB(),\n",
    "          'mnb': MultinomialNB(),\n",
    "          'gbc' : GradientBoostingClassifier()\n",
    "         }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# pred_probs = {}\n",
    "# scores = {}\n",
    "# preds = {}\n",
    "# for mname, m in models.iteritems():\n",
    "X = StandardScaler().fit_transform(X)\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "\n",
    "#     print \"*** %s\" % mname\n",
    "#     m.fit(X_train, y_train)\n",
    "#     if mname != 'linsvc':\n",
    "#         pred_probs[mname] = {'train': m.predict_proba(X_train),  'test': m.predict_proba(X_test)}\n",
    "#     pred = m.predict(X_test)\n",
    "#     preds[mname] = pred\n",
    "#     prec, recall, fscore, sup = precision_recall_fscore_support(y_test, pred)\n",
    "#     scores[mname] = {'accuracy': accuracy_score(y_test, pred),\n",
    "#                      'precision': prec,\n",
    "#                      'recall': recall,\n",
    "#                      'fscore': fscore}\n",
    "# pprint(scores)\n",
    "\n",
    "# for mname, y_pred in preds.iteritems():\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 3)\n",
    "#     ax = sns.heatmap(cm, annot=True, xticklabels=names, yticklabels=names)\n",
    "#     plt.yticks(rotation=0) \n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.title(\"%s confusion matrix, %s\" %(mname, topic))\n",
    "#     sns.plt.show()\n",
    "\n",
    "### coefs\n",
    "\n",
    "def get_scores_coef(X, y):\n",
    "    # X_train,X_test,y_train,y_test = train_test_split(X ,y,test_size=0.3, random_state=42)\n",
    "    for mname, m in model_dict.iteritems():\n",
    "\n",
    "        coefs = sorted(zip(m.coef_[0], X_train.columns))\n",
    "        for coef in coefs:\n",
    "            print '%.05f \\t%s' % (coef)\n",
    "        all_preds[mname] = preds\n",
    "        all_proba[mname] = proba\n",
    "        return coefs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 MVP Approach: Just predict the winners, regardless of race\n",
    "1. Try all classifiers, optimizing when necessary \n",
    "2. Evaluate using all metrics \n",
    "    -roc curve, all thresholds \n",
    "    -percision, accuracy, etc. \n",
    "    -knn\n",
    "        -number of ks\n",
    "    -log\n",
    "        -logloss\n",
    "## Decisions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_samples = X_train.shape[0]\n",
    "log = LogisticRegression()\n",
    "# cv = cross_validation.ShuffleSplit(n_samples, n_iter=10, test_size=0.25, random_state=1)\n",
    "# log.fit(X_t)\n",
    "# scores = cross_validation.cross_val_score(log, X_train, y_train.ravel(), cv=cv)\n",
    "y = df['winner']\n",
    "X = df[['total' , 'incumbent', 'party', 'congyear']]\n",
    "\n",
    "y_pred_proba = cross_validation.cross_val_predict(log, X, y, cv=10)\n",
    "# fpr, tpr, _ = sklearn.metrics.roc_curve(y_test, y_pred_proba[:,1])\n",
    "# roc_rates = [fpr, tpr]\n",
    "# auc = sklearn.metrics.auc(fpr, tpr)\n",
    "# plt.title(\"Logistic Regression\", fontsize = 20)\n",
    "# plt.xlabel(\"False positive rate\")\n",
    "# plt.ylabel(\"True positive rate\")\n",
    "# labelname = \"Logistic Regression\" + \", (area = %0.3f)\" % auc\n",
    "# plt.plot(roc_rates[0], roc_rates[1], linewidth = 2, label = labelname)\n",
    "# plt.legend(loc = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65417738169095463"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score(model = model(), X=X, y=y, cv=5):\n",
    "    mod = model\n",
    "    scores = cross_validation.cross_val_score(\n",
    "        mod, X, y, cv=cv)\n",
    "    return scores \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Pick winner based on opponent, whomever has the greater probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Train as pairs, take into account opponent characterists \n",
    "### Append opponent data to candidate variables ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Graphs\n",
    "scatter marked winner/loser\n",
    "-small/large\n",
    "-crosstables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtab = pd.crosstab(np.array(df['cpo']), np.array(df['winner']), rownames=['cpo'], colnames=['outcome'])[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = X_train.shape[0]\n",
    "log = LogisticRegression()\n",
    "cv = cross_validation.ShuffleSplit(n_samples, n_iter=10, test_size=0.3, random_state=1)\n",
    "scores = cross_validation.cross_val_score(log, X_train, y_train.ravel(), cv=cv)\n",
    "# print scores.mean()\n",
    "log.fit(X_train, y_train.ravel())\n",
    "print \"Accuracy Score: %f\" % log.score(X_test, y_test.ravel())\n",
    "\n",
    "log.fit(X_train, y_train.ravel())\n",
    "y_pred = log.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "labelname = log.classes_\n",
    "sns.heatmap(cm, annot=True,  fmt='', xticklabels=labelname, yticklabels=labelname)\n",
    "\n",
    "###\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# X = df[['incumbent', 'open', 'challenger', 'percunitem', 'congyear', 'total_sc', 'party', 'primaryperc',\n",
    "#         'close2', 'close5', 'close10', 'leader', 'chairman', 'majorityprev', 'vap', 'adm', 'agr', 'app', 'arm', 'ban', 'bud', \n",
    "#              'bus', 'ecn', 'edu', 'egw', \n",
    "#              'ene', 'for', 'gov', 'hsc', 'int', 'jud',\n",
    "#              'lib', 'nat', 'pri', 'rul', 'sci', 'sta',\n",
    "# #              'tax', 'tra', 'vet', 'way', 'ieagainst', 'ieproxy', 'percpacs', 'ie', 'percind']]\n",
    "# X = df[['ieagainst']]\n",
    "# y = df['winner']\n",
    "\n",
    "labels = ['Winner', \"Loser\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "classifier = GradientBoostingClassifier()\n",
    "y_pred = classifier.fit(X, y).predict(X)\n",
    "\n",
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(y))\n",
    "    ax.set_xticklabels([''] + labels)\n",
    "    ax.set_yticklabels([''] + labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y, y_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "fig = plt.figure()\n",
    "fig.patch.set_alpha(0.5)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.patch.set_alpha(0.5)\n",
    "cax = ax.matshow(cm)\n",
    "plot_confusion_matrix(cm)\n",
    "\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "\n",
    "plt.show()\n",
    "plt.savefig('temp.png', transparent=True)\n",
    "\n",
    "###\n",
    "def print_cm(cm, labels, hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n",
    "    \"\"\"pretty print for confusion matrixes\"\"\"\n",
    "    columnwidth = max([len(x) for x in labels]) # 5 is value length\n",
    "    empty_cell = \" \" * columnwidth\n",
    "    # Print header\n",
    "    print \"    \" + empty_cell,\n",
    "    for label in labels: \n",
    "        print \"%{0}s\".format(columnwidth) % label,\n",
    "    print\n",
    "    # Print rows\n",
    "    for i, label1 in enumerate(labels):\n",
    "        print \"    %{0}s\".format(columnwidth) % label1,\n",
    "        for j in range(len(labels)): \n",
    "            cell = \"%{0}.1f\".format(columnwidth) % cm[i, j]\n",
    "            if hide_zeroes:\n",
    "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n",
    "            if hide_diagonal:\n",
    "                cell = cell if i != j else empty_cell\n",
    "            if hide_threshold:\n",
    "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n",
    "            print cell,\n",
    "        print\n",
    "\n",
    "\n",
    "labels = [\"1\", \"2\", \"3\", \"4\"]\n",
    "cm = confusion_matrix(y_pred, y, labels)\n",
    "\n",
    "\n",
    "print_cm(cm, labels)\n",
    "\n",
    "###\n",
    "for mname, y_pred in preds.iteritems():\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm = np.round(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis], 3)\n",
    "    ax = sns.heatmap(cm, annot=True, xticklabels=names, yticklabels=names)\n",
    "    plt.yticks(rotation=0) \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title(\"%s confusion matrix, %s\" %(mname, topic))\n",
    "    sns.plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whatever this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = pd.get_dummies(df['cpo']).join(df[['party', 'congyear', 'unitemtotal', 'total', 'total_sc']])\n",
    "y = df['winner']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=46)\n",
    "log = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred_proba = log.predict_proba(X_test)\n",
    "fpr, tpr, _ = roc_curve()\n",
    "roc_rates = [fpr, tpr]\n",
    "auc = auc(fpr, tpr)\n",
    "plt.title(\"Logistic Regression\", fontsize = 20)\n",
    "plt.xlabel(\"False positive rate\")\n",
    "plt.ylabel(\"True positive rate\")\n",
    "labelname = \"Logistic Regression\" + \", (area = %0.3f)\" % auc\n",
    "plt.plot(roc_rates[0], roc_rates[1], linewidth = 2, label = labelname)\n",
    "plt.legend(loc = 4)\n",
    "log_loss(y_test, y_pred_proba[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#WRONG\n",
    "sns.set_style(\"white\")\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "ax = fig.add_subplot(111) \n",
    "m = GradientBoostingClassifier()\n",
    "m.fit(X_train, y_train)\n",
    "y_pred = m.predict(X_test)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "\n",
    "ax.set_ylabel('True Positive Rate', fontsize = 15)\n",
    "ax.set_xlabel('False Positive Rate', fontsize=15)\n",
    "plt.plot(fpr, tpr, color = '#9ecae1')\n",
    "plt.plot([0, 1], [0, 1], color = '#08306b', linestyle = '--')\n",
    "plt.show()\n",
    "plt.savefig('demo.png', transparent=True)\n",
    "\n",
    "###\n",
    "estimators = [\n",
    "    LogisticRegression(), \n",
    "    SVC(probability=True), \n",
    "    GaussianNB(), \n",
    "    DecisionTreeClassifier(max_depth = 5), \n",
    "    RandomForestClassifier(), \n",
    "    KNeighborsClassifier(n_neighbors = 10)\n",
    "    ]\n",
    "\n",
    "names = [\"Logistic Regression\", \"Support Vector Machine\", \"Gaussian Naive Bayes\", \"Decision Tree Classifier\",\n",
    "        \"Random Forest Classifier\", \"K Neighbors - 10\"]\n",
    "\n",
    "colors = [\"b\", \"g\", \"r\", \"k\", \"c\", \"y\"]\n",
    "\n",
    "def roc_rates(estimator, X_train = X_train, y_train = y_train.ravel(), X_test = X_test, y_test = y_test.ravel()):\n",
    "    \n",
    "    #Recursive Features Selection\n",
    "    model = estimator\n",
    "    model.fit(X_train, y_train)\n",
    "    test_pred = model.predict_proba(X_test)\n",
    "    \n",
    "    # return the roc parameters\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, test_pred[:,1])\n",
    "    roc_rates = [fpr, tpr]\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return roc_rates, auc\n",
    "\n",
    "#plt.figure(figsize = (15,12))\n",
    "\n",
    "for i, model in enumerate(estimators):\n",
    "    #plt.subplot(2,3, i)\n",
    "    #plt.title(names[i], fontsize = 20)\n",
    "    plt.xlabel(\"False positive rate\")\n",
    "    plt.ylabel(\"True positive rate\")\n",
    "    \n",
    "    # plot parameters using unscaled predictors\n",
    "    params = roc_rates(estimators[i], X_train, y_train.ravel(), X_test, y_test.ravel())[0]\n",
    "    area = roc_rates(estimators[i], X_train, y_train.ravel(), X_test, y_test.ravel())[1]\n",
    "    labelname = names[i] + \", (area = %0.2f)\" % area\n",
    "    plt.plot(params[0], params[1], colors[i] + \"--\", linewidth = 2, label = labelname)\n",
    "    plt.legend(loc = 4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate log_loss\n",
    "for i, estimator in enumerate(estimators):\n",
    "    model = estimator\n",
    "    model.fit(X_train, y_train.ravel())\n",
    "    y_pred = model.predict_proba(X_test)\n",
    "    ll = metrics.log_loss(y_test.ravel(), y_pred[:,1])\n",
    "    print names[i] + ', LogLoss: %0.2f'% ll"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
